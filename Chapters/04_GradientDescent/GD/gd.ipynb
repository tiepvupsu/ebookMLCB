{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các bạn hẳn thấy hình vẽ dưới đây quen thuộc:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/gradient_descent.png?raw=true\" align = \"center\" width = \"600\">\n",
    "</div>\n",
    "\n",
    "Điểm màu xanh lục là điểm local minimum (cực tiểu), và cũng là điểm _global minimum_, của hàm số \\\\(f(x) = \\frac{1}{2}(x-1)^2 - 2\\\\). Từ đây trở đi, tôi sẽ dùng _local minimum_ để thay cho _điểm cực tiểu_, _global minimum_ để thay cho _điểm mà tại đó hàm số đạt giá trị nhỏ nhất_. \n",
    "\n",
    "Giả sử chúng ta đang quan tâm đến một hàm số một biến có đạo hàm mọi nơi. Xin cho tôi được nhắc lại vài điều đã quá quen thuộc:\n",
    "\n",
    "1. Điểm cực tiểu \\\\(x^\\*\\\\) của hàm số là điểm có đạo hàm \\\\(f'(x(t))\\\\) bằng 0. Hơn thế nữa, trong lân cận của nó, đạo hàm của các điểm phía bên trái \\\\(x^*\\\\) là không dương, đạo hàm của các điểm phía bên phải \\\\(x^*\\\\) là không âm.\n",
    "2. Đường tiếp tuyến với đồ thị hàm số đó tại 1 điểm bất kỳ có hệ số góc chính bằng đạo hàm của hàm số tại điểm đó. \n",
    "\n",
    "Trong hình phía trên, các điểm bên trái của điểm cực tiểu màu xanh lục có đạo hàm âm, các điểm bên phải có đạo hàm âm. Và đối với hàm số này, càng xa về phía trái của điểm cực tiểu thì đạo hàm càng âm, càng xa về phía phải thì đạo hàm càng âm. \n",
    "\n",
    "### Gradient Descent\n",
    "Trong Machine Learning nói riêng và Toán Tối Ưu nói chung, chúng ta thường xuyên phải tìm giá trị nhỏ nhất (hoặc đôi khi là lớn nhất) của một hàm số nào đó. Ví dụ như các hàm mất mát trong hai bài [Linear Regression](/2016/12/28/linearregression/) và [K-means Clustering](/2017/01/01/kmeans/). Nhìn chung, việc tìm giá trị nhỏ nhất (global minimum) của hàm số là không khả thi trong các bài toán này. Thay vào đó, người ta thường cố gắng tìm các điểm cực tiểu (local minimum), và ở một mức độ nào đó, coi đó là nghiệm cần tìm của bài toán. Các điểm cực tiểu là nghiệm của phương trình đạo hàm bằng 0. Nếu bằng một cách nào đó có thể tìm được toàn bộ (hữu hạn) các điểm cực tiểu, ta chỉ cần thay từng điểm cực tiểu đó vào hàm số rồi tìm điểm làm cho hàm có giá trị nhỏ nhất (_đoạn này nghe rất quen thuộc, đúng không?_). Tuy nhiên, trong hầu hết các trường hợp, việc giải phương trình đạo hàm bằng 0 trong thực tế vẫn là bất khả thi. Nguyên nhân có thể đến từ sự phức tạp của dạng của đạo hàm, từ việc các điểm dữ liệu có số chiều lớn, hoặc từ việc có quá nhiều điểm dữ liệu. \n",
    "\n",
    "Hướng tiếp cận phổ biến nhất là xuất phát từ một điểm mà chúng ta coi là _gần_ với nghiệm của bài toán, sau đó dùng một phép toán lặp để tiến dần đến điểm cần tìm, tức đến khi đạo hàm gần với 0. (Đây cũng chính là lý do phương pháp này được gọi là Gradient Descent - tức giảm đạo hàm). Gradient Descent (viết gọn là GD) và các biến thể của nó là một phương pháp được dùng nhiều nhất. \n",
    "\n",
    "## Gradient Descent cho hàm 1 biến\n",
    "Quay trở lại hình vẽ ban đầu và một vài quan sát tôi đã nêu. Giả sử  \\\\(x^\\*\\\\) là điểm global minimum màu xanh lục, \\\\(x(t)\\\\) là điểm ta tìm được sau vòng lặp thứ \\\\(t\\\\). Ta cần tìm một thuật toán để đưa \\\\(x(t)\\\\) về càng gần \\\\(x^\\*\\\\) càng tốt. \n",
    "\n",
    "Chúng ta lại có thêm hai quan sát nữa:\n",
    "\n",
    "1. Nếu đạo hàm của hàm số tại \\\\(x(t)\\\\): \\\\(f'(x(t)) > 0\\\\) thì \\\\(x(t)\\\\) nằm về bên phải so với \\\\(x^*\\\\) (và ngược lại). Để điểm tiếp theo \\\\(x(t+1)\\\\) gần với \\\\(x^\\*\\\\) hơn, chúng ta cần di chuyển \\\\(x(t)\\\\) về phía bên trái, tức về phía _âm_. Nói các khác, __chúng ta cần di chuyển ngược dấu với đạo hàm__:\n",
    "\\\\[\n",
    "x(t+1) = x(t) + \\delta\n",
    "\\\\]\n",
    "Trong đó \\\\(\\delta\\\\) ngược dấu với đạo hàm.\n",
    "\n",
    "2. \\\\(f'(x(t))\\\\) càng lớn hơn 0 thì \\\\(x(t)\\\\) càng xa \\\\(x^*\\\\) về phía bên phải (và ngược lại). Vậy, lượng di chuyển \\delta, một cách trực quan nhất, là tỉ lệ thuận với \\\\(-f'(x(t))\\\\). \n",
    "\n",
    "Hai nhận xét phía trên cho chúng ta một các cập nhật đơn giản là:\n",
    "\\\\[\n",
    "x(t+1) = x(t) - \\eta f'(x(t))\n",
    "\\\\]\n",
    "\n",
    "Trong đó \\\\(\\eta\\\\) (đọc là _eta_) là một số dương được gọi là _learning rate_ (tốc độ học). Dấu trừ thể hiện việc chúng ta phải đi ngược với đạo hàm. \n",
    "\n",
    "### Ví dụ đơn giản với Python\n",
    "\n",
    "Xét hàm số \\\\(f(x) = x^2 + 5\\sin(x)\\\\) với đạo hàm \\\\(f'(x) = 2x + 5\\cos(x)\\\\) (tôi chọn hàm này vì nó không dễ tìm nghiệm của đạo hàm bằng 0 như hàm phía trên). Giả sử bắt đầu từ một điểm \\\\(x(0)\\\\) nào đó, tại vòng lặp thứ \\\\(t\\\\), chúng ta sẽ cập nhật như sau:\n",
    "\\\\[\n",
    "x(t+1) = x(t) - \\eta(2x + 5\\cos(x))\n",
    "\\\\]\n",
    "\n",
    "Như thường lệ, tôi khai báo vài thư viện quen thuộc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import math\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, tôi viết các hàm số :\n",
    "\n",
    "1. `grad` để tính đạo hàm\n",
    "2. `cost` để tính giá trị của hàm số. Hàm này không sử dụng trong thuật toán nhưng thường được dùng để kiểm tra việc tính đạo hàm của đúng không hoặc để xem giá trị của hàm số có giảm theo mỗi vòng lặp hay không.\n",
    "3. `myGD1` là phần chính thực hiện thuật toán Gradient Desent nêu phía trên. Thuật toán dừng lại khi đạo hàm đủ nhỏ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return 2*x+ 5*np.cos(x)\n",
    "\n",
    "def cost(x):\n",
    "    return x**2 + 5*np.sin(x)\n",
    "\n",
    "def myGD1(eta, x0):\n",
    "    x = [x0]\n",
    "    for it in range(100):\n",
    "        x_new = x[-1] - eta*grad(x[-1])\n",
    "        if abs(grad(x_new)) < 1e-3:\n",
    "            break\n",
    "        x.append(x_new)\n",
    "    return (x, it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi có các hàm cần thiết, tôi thử tìm nghiệm với các điểm ban đầu khác nhau là \\\\(x(0) = -5\\\\) và \\\\(x(0) = 5\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x1 = -1.110667, cost = -3.246394, obtained after 11 iterations\n",
      "Solution x2 = -1.110341, cost = -3.246394, obtained after 29 iterations\n"
     ]
    }
   ],
   "source": [
    "(x1, it1) = myGD1(.1, -5)\n",
    "(x2, it2) = myGD1(.1, 5)\n",
    "print('Solution x1 = %f, cost = %f, obtained after %d iterations'%(x1[-1], cost(x1[-1]), it1))\n",
    "print('Solution x2 = %f, cost = %f, obtained after %d iterations'%(x2[-1], cost(x2[-1]), it2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy là với các điểm ban đầu khác nhau, thuật toán của chúng ta tìm được nghiệm giống nhau, mặc dù với tốc độ hội tụ khác nhau. Dưới đây là hình ảnh minh họa thuật toán GD cho bài toán này.\n",
    "\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"50%\" style = \"border: 0px solid white\"> \n",
    "            <img src = \"/assets/GD/1dimg_5_0.1_-5.gif\">\n",
    "        </td>\n",
    "        <td width=\"50%\" style = \"border: 0px solid white\">\n",
    "            <img src = \"/assets/GD/1dimg_5_0.1_5.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "Từ hình minh họa trên ta thấy rằng bên trái hội tụ nhanh hơn (\\\\(x(0) = -5\\\\) vì nó gần với nghiệm hơn. Hơn nữa, khi (\\\\(x(0) = -5 \\\\), _đường đi_ của nó phải đi qua một điểm có hoành độ gần bằng 2, điểm này có đạo hàm khá nhỏ (nhưng khác 0), khiến cho thuật toán _la cà_ ở đây khá lâu. Khi vượt qua được điểm này thì mọi việc diễn ra rất tốt đẹp. \n",
    "\n",
    "Tốc độ hội tụ của GD không những phụ thuộc vào điểm khởi tạo ban đầu mà còn phụ thuộc vào _learning rate_. Dưới đây là một ví dụ với cùng điểm khởi tạo \\\\(x(0) = -5\\\\) nhưng tốc độ học khác nhau:\n",
    "\n",
    "Ta quan sát thấy hai điều:\n",
    "\n",
    "1. Với _learning rate_ nhỏ \\\\(x(0) = 0.01\\\\), tốc độ hội tụ rất chậm. Trong ví dụ này tôi chọn tối đa 100 vòng lặp nên thuật toán dừng lại trước khi tới _đích_, mặc dù đã rất gần. Trong thực tế, khi việc tính toán trở nên phức tạp, _learning rate_ quá thấp sẽ ảnh hưởng tới tốc độ của thuật toán rất nhiều, thậm chí không bao giờ tới được đích. \n",
    "2. Với _learning rate_ lớn \\\\(x(0) = 0.5\\\\), thuật toán tiến rất nhanh tới _gần đích_. Tuy nhiên, thuật toán không hội tụ được vì _bước nhảy_ quá lớn, khiến nó cứ _quẩn quanh_ ở đích. \n",
    "\n",
    "Việc lựa chọn _learning rate_ rất quan trọng trong các bài toán thực tế. Việc lựa chọn giá trị này phụ thuộc nhiều vào từng bài toán và phải làm một vài thí nghiệm để chọn ra giá trị tốt nhất. Ngoài ra, có một vài phương pháp lựa chọn _learning rate_, hoặc _learning rate_ khác nhau sau mỗi vòng lặp. Tôi sẽ quay lại vấn đề này ở phần sau. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent cho hàm nhiều biến\n",
    "Giả sử ta cần tìm global minimum cho hàm \\\\(f(\\mathbf{\\theta}\\\\) trong đó \\\\(\\mathbf{\\theta}\\\\) (_theta_) là một vector, thường được dùng để ký hiệu tập hợp các tham số của một mô hình cần tối ưu. Đạo hàm (gradient) của hàm số đó tại một điểm \\\\(\\theta\\\\) bất kỳ được ký hiệu là \\\\(\\nabla_{\\theta}f(\\theta)\\\\) (hình tam giác ngược đọc là _nabla_). Tương tự như hàm 1 biến, thuật toán GD cho hàm nhiều biến cũng bắt đầu bằng một điểm dự đoán \\\\(\\theta(0)\\\\), sau đó, ở vòng lặp thứ \\\\(t\\\\), quy tắc cập nhật là:\n",
    "\n",
    "\\\\[\n",
    "\\theta(t+1) = \\theta(t) - \\eta \\nabla_{\\theta} f(\\theta(t))\n",
    "\\\\]\n",
    "\n",
    "Hoặc viết dưới dạng đơn giản hơn: \\\\(\\theta = \\theta - \\eta \\nabla_{\\theta} f(\\theta)\\\\)\n",
    "\n",
    "### Quay lại với bài toán Linear Regression\n",
    "Trong mục này, chúng ta quay lại với bài toán [Linear Regression](/2016/12/28/linearregression/) và thử tối ưu hàm mất mát của nó bằng thuật toán GD. \n",
    "\n",
    "Hàm mất mát của Linear Regression là: \n",
    "\\\\[\n",
    "\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2N}\\|\\|\\mathbf{y - \\bar{X}w}\\|\\|_2^2\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chú ý**: hàm này có khác một chút so với hàm tôi nêu trong bài [Linear Regression](/2016/12/28/linearregression/). Mẫu số có thêm \\\\(N\\\\), tức lấy trung bình cộng, để tránh trường hợp hàm mất mát và đạo hàm có giá trị là một số rất lớn, ảnh hưởng tới khả năng tính toán. Về mặt toán học, nghiệm của hai bài toán là như nhau.\n",
    "\n",
    "Đạo hàm của hàm mất mát là:\n",
    "\\\\[\n",
    "\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}) = \n",
    "\\frac{1}{N}\\mathbf{\\bar{X}}^T \\mathbf{(\\bar{X}w - y)}\n",
    "\\\\]\n",
    "\n",
    "### Sau đây là ví dụ trên Python và một vài lưu ý khi lập trình\n",
    "\n",
    "1. Load thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta tạo dữ liệu 1000 điểm dữ liệu được chọn _gần_ với đường thẳng \\\\(y = 4 + 3x\\\\), hiển thị chúng và tìm nghiệm theo công thức:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution found by formula: w =  [[ 4.00305242  2.99862665]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAFkCAYAAAC0KZhSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d600f6250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.random.rand(1000, 1)\n",
    "y = 4 + 3 * X + .2*np.random.randn(1000, 1)\n",
    "\n",
    "# Building Xbar \n",
    "one = np.ones((X.shape[0],1))\n",
    "Xbar = np.concatenate((one, X), axis = 1)\n",
    "\n",
    "A = np.dot(Xbar.T, Xbar)\n",
    "b = np.dot(Xbar.T, y)\n",
    "w_lr = np.dot(np.linalg.pinv(A), b)\n",
    "print('Solution found by formula: w = ',w_lr.T)\n",
    "\n",
    "w = w_lr\n",
    "w_0 = w[0][0]\n",
    "w_1 = w[1][0]\n",
    "x0 = np.linspace(0, 1, 2, endpoint=True)\n",
    "y0 = w_0 + w_1*x0\n",
    "\n",
    "# Drawing the fitting line \n",
    "plt.plot(X.T, y.T, 'b.')     # data \n",
    "plt.plot(x0, y0, 'y', linewidth = 2)               # the fitting line\n",
    "plt.axis([0, 1, 0, 10])\n",
    "# plt.xlabel('(cm)')\n",
    "# plt.ylabel('Weight (kg)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đường thẳng tìm được là đường có màu vàng có phương trình \\\\(y \\approx 4 + 2.998x\\\\).\n",
    "\n",
    "Tiếp theo ta viết đạo hàm và hàm mất mát:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w):\n",
    "    N = Xbar.shape[0]\n",
    "    return .5/N*np.linalg.norm(y - Xbar.dot(w), 2)**2;\n",
    "\n",
    "def grad(w):\n",
    "    N = Xbar.shape[0]\n",
    "    return 1/N * Xbar.T.dot(Xbar.dot(w) - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kiểm tra đạo hàm\n",
    "Việc tính đạo hàm có hàm nhiều biến thông thường khá phức tạp, nếu chúng ta tính sai đạo hàm thì thuật toán GD không thể chạy đúng được. Trong thực nghiệm, có một cách để kiểm tra liệu đạo hàm tính được có chính xác không. Cách này dựa trên định nghĩa của đạo hàm (cho hàm 1 biến):\n",
    "\\\\[\n",
    "f'(x) = \\lim_{\\epsilon \\rightarrow 0}\\frac{f(x + \\epsilon) - f(x)}{\\epsilon}\n",
    "\\\\]\n",
    "\n",
    "Một cách thường được sử dụng là lấy một giá trị \\\\(\\epsilon \\\\) rất nhỏ, ví dụ \\\\(10^{-6}\\\\), và sử dụng công thức:\n",
    "\\\\[\n",
    "f'(x) \\approx \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon} ~~~~ (1)\n",
    "\\\\]\n",
    "\n",
    "Cách tính này được gọi là _numerical gradient_.\n",
    "\n",
    "Với hàm nhiều biến, công thức \\\\((1)\\\\) được áp dụng cho từng biến khi các biến khác cố định. Cách tính này thường cho giá trị khá chính xác. Tuy nhiên, cách này không được sử dụng để tính đạo hàm vì độ phức tạp quá cao so với cách tính trực tiếp. Khi so sánh đạo hàm này với đạo hàm chính xác tính theo công thức, người ta thường giảm số chiều dữ liệu và giảm số điểm dữ liệu để thuận tiện cho tính toán. Một khi đạo hàm tính được rất gần với _numerical gradient_, chúng ta có thể tự tin là đạo hàm tính được là chính xác.\n",
    "\n",
    "Dưới đây là một đoạn code đơn giản để kiểm tra đạo hàm và có thể áp dụng với một hàm số (của một vector) bất kỳ với `cost` và `grad` đã tính ở phía trên. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient... True\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad(w, cost):\n",
    "\teps = 1e-4\n",
    "\tg = np.zeros_like(w)\n",
    "\tfor i in range(len(w)):\n",
    "\t\tw_p = w.copy()\n",
    "\t\tw_n = w.copy()\n",
    "\t\tw_p[i] += eps \n",
    "\t\tw_n[i] -= eps\n",
    "\t\tg[i] = (cost(w_p) - cost(w_n))/(2*eps)\n",
    "\treturn g \n",
    "\n",
    "def check_grad(w, cost, grad):\n",
    "\tw = np.random.rand(w.shape[0], w.shape[1])\n",
    "\tgrad1 = grad(w)\n",
    "\tgrad2 = numerical_grad(w, cost)\n",
    "\treturn True if np.linalg.norm(grad1 - grad2) < 1e-6 else False \n",
    "\n",
    "\n",
    "print( 'Checking gradient...', check_grad(np.random.rand(2, 1), cost, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với bài toán Linear Regression, cách tính đạo hàm phía trên được coi là đúng vì sai số giữa hai cách tính nhỏ (nhỏ hơn \\\\(10^{-6}\\\\)). Sau khi có được đạo hàm chính xác, chúng ta viết hàm cho GD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution found by GD: w =  [[ 4.01780793  2.97133693]] ,\n",
      "after %d iterations 49\n"
     ]
    }
   ],
   "source": [
    "def myGD(w_init, grad, eta):\n",
    "\tw = [w_init]\n",
    "\tfor it in range(100):\n",
    "\t\tw_new = w[-1] - eta*grad(w[-1])\n",
    "\t\tif np.linalg.norm(grad(w_new))/len(w_new) < 1e-3:\n",
    "\t\t\tbreak \n",
    "\t\tw.append(w_new)\n",
    "\t\t# print('iter %d: ' % it, w[-1].T)\n",
    "\treturn (w, it) \n",
    "\n",
    "w_init = np.array([[2], [1]])\n",
    "(w1, it1) = myGD(w_init, grad, 1)\n",
    "print('Solution found by GD: w = ', w1[-1].T, ',\\nafter %d iterations', it1+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau 49 vòng lặp, thuật toán đã hội tụ với một nghiệm khá gần với nghiệm tìm được theo công thức. \n",
    "\n",
    "Dưới đây là hình động minh họa thuật toán GD.\n",
    "\n",
    "\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\"> \n",
    "        <img src = \"/assets/GD/img1_1.gif\">\n",
    "         </td>\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\">\n",
    "        <img src = \"/assets/GD/img2_1.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "Trong hình bên trái, các đường thẳng màu đỏ là nghiệm tìm được sau mỗi vòng lặp. \n",
    "\n",
    "Trong hình bên phải, tôi xin giới thiệu một thuật ngữ mới:\n",
    "#### Đường đồng mức (level sets)\n",
    "Độ thị của một hàm số với hai biến đầu vào cần được vẽ trong không gian ba chiều, nhều khi không giúp chúng ta nhìn rõ được nghiệm có khoảng tọa độ bao nhiêu. Trong toán tối ưu, người ta thường dùng một cách vẽ hình chiếu sử dụng khái niệm _đường đồng mức_ (level sets). \n",
    "\n",
    "Nếu các bạn để ý trong các bản độ tự nhiên, để miêu tả độ cao của các dãy núi, người ta dùng nhiều đường con kín bao quanh nhau như sau:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"http://files.vforum.vn/2016/T06/img/vforum.vn-324944-hinh-44-lc6b0e1bba3c-c491e1bb93-c491e1bb8ba-hc3acnh-te1bb89-le1bb87-le1bb9bn.png\" align = \"center\" width = \"600\">\n",
    " <div class = \"thecap\"> Ví dụ về đường đồng mức trong các bản đồ tự nhiên. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các vòng nhỏ màu đỏ hơn thể hiện các điểm ở trên cao hơn. \n",
    "\n",
    "Trong toán tối ưu, người ta cũng dùng phương pháp này để thể hiện các đồ thị trong không gian ba chiều trong không gian hai chiều. \n",
    "\n",
    "Quay trở lại với hình minh họa thuật toán GD cho bài toán Liner Regression bên trên, hình bên phải là hình biểu diễn các level sets. Tức là tại các điểm trên cùng một vòng, hàm mất mát có giá trị như nhau. Trong ví dụ này, tôi hiển thị giá trị của hàm số tại một số vòng. Các vòng màu xanh có giá trị thấp, các vòng tròn màu đỏ phía ngoài có giá trị cao hơn. Điểm này khác một chút so với đường đồng mức trong tự nhiên là các vòng bên trong thường thể hiện một thung lũng hơn là một đỉnh núi (vì chúng ta đang đi tìm giá trị nhỏ nhất).\n",
    "\n",
    "Tôi thử với _learning rate_ nhỏ hơn, kết quả như sau:\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\"> \n",
    "        <img src = \"/assets/GD/img1_0.1.gif\">\n",
    "         </td>\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\">\n",
    "        <img src = \"/assets/GD/img2_0.1.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tốc độ hội tụ đã chậm đi nhiều, thậm chí sau 99 vòng lặp, GD vân chưa tìm được nghiệm tốt nhất. Trong các bài toán thực tế, chúng ta cần nhiều vòng lặp hơn nhiều, vì số chiều và số điểm dữ liệu là rất lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Một ví dụ khác\n",
    "\n",
    "Để kết thúc phần 1 của Gradient Descent, tôi xin nêu thêm một ví dụ khác.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"/assets/GD/img3_0.015.gif\" align = \"center\" width = \"800\">\n",
    "</div>\n",
    "\n",
    "Hàm số \\\\(f(x, y) = (x^2 + y - 7)^2 + (x - y + 1)^2\\\\) có hai điểm local minimum tại \\\\((2, 3)\\\\) và \\\\(-3, -2)\\\\), và chúng cũng là hai điểm global minimum. Trong ví dụ này, tùy vào điểm khởi tạo mà chúng ta thu được các nghiệm cuối cùng khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thảo luận\n",
    "\n",
    "Dựa trên GD, có rất nhiều thuật toán phức tạp và hiệu quả hơn được thiết kế cho những loại bài toán khác nhau. Vì bài này đã đủ dài, tôi xin phép dừng lại ở đây. Mời các bạn đón đọc bài Gradient Descent phần 2 với nhiều kỹ thuật nâng cao hơn.\n",
    "\n",
    "## Tài liệu tham khảo\n",
    "1. [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)\n",
    "2. [http://www.benfrederickson.com/numerical-optimization/](An Interactive Tutorial on Numerical Optimization)\n",
    "3. [Gradient Descent by Andrew NG](https://www.youtube.com/watch?v=eikJboPQDT0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
