{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong [phần 1](/2017/01/12/gradientdescent/) của Gradient Descent (GD), tôi đã giới thiệu với bạn đọc về thuật toán Gradient Descent. Tôi xin nhắc lại rằng nghiệm cuối cùng của Gradient Descent phụ thuộc vào điểm khởi tạo và learning rate. Trong bài này, tôi xin đề cập một vài phương pháp thường được dùng để khắc phục những hạn chế của GD . Đồng thời, các thuật toán biến thể của GD thường được áp dụng trong các mô hình Deep Learning cũng sẽ được tổng hợp. \n",
    "\n",
    "**Trong trang này:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các thuật toán tối ưu Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "#### Nhắc lại thuạt toán Gradient Descent\n",
    "Dành cho các bạn chưa đọc [phần 1](/2017/01/12/gradientdescent/) của Gradient Descent. Để giải bài toán tìm điểm _global optimal_ của hàm mất mát \\\\(J(\\theta)\\\\) (Hàm mất mát cũng thường được ký hiệu là \\\\(J()\\\\) với \\\\(\\theta\\\\) là tập hợp các tham số của mô hình, thuật tóan GD được phát biểu như sau:\n",
    "\n",
    " --  --  --  --  --  -- -\n",
    "** Thuật toán Gradient Descent:**\n",
    "1. Dự đoán một điểm khởi tạo \\\\(\\theta = \\theta_0\\\\).\n",
    "2. Cập nhật \\\\(\\theta\\\\) đến khi đạt được kết quả chấp nhận được: \n",
    "\\\\[\n",
    "\\theta = \\theta - \\eta \\nabla_{\\theta}J(\\theta)\n",
    "\\\\]\n",
    "\n",
    "với \\\\(\\nabla_{\\theta}J(\\theta)\\\\) là đạo hàm của hàm mất mát tại \\\\(\\theta\\\\).\n",
    "\n",
    " --  --  --  --  --  -- \n",
    "\n",
    "#### Gradient dưới góc nhìn vật lý \n",
    "\n",
    "Thuật toán GD thường được ví với tác dụng của trọng lực lên một hòn bi đặt trên một mặt có dạng như hình một thung lũng giống như hình 1a) dưới đây. Khi có một điểm ở đáy thung lũng như điểm C, bất kể ta đặt hòn bi ở A hay B thì cuối cùng hòn bi cũng sẽ lăn xuống và kết thúc ở vị trí C.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"/assets/GD/momentum.png\" align = \"center\" width = \"800\">\n",
    "</div>\n",
    "\n",
    "Tuy nhiên, nếu như mặt phẳng có hai đáy thung lũng như Hình 1b) thì tùy vào việc đặt bi ở A hay B, vị trí cuối cùng của bi sẽ ở C hoặc D. Điểm D là một điểm local minimum chúng ta không mong muốn. \n",
    "\n",
    "Nếu suy nghĩ một cách vật lý hơn, vẫn trong Hình 1b), nếu vận tốc ban đầu của bi khi ở điểm B đủ lớn, khi bi lăn đến điểm D, theo _đà_, bi có thể tiếp tục di chuyển lên dốc phía bên trái của D. Và nếu vận tốc ban đầu lớn hơn nữa, bi có thể vượt dốc tới điểm E rồi lăn xuống C như trong Hình 1c). Đây chính là điều chúng ta mong muốn. Bạn đọc có thể đặt câu hỏi rằng liệu bi lăn từ A tới C có theo _đà_ lăn tới E rồi tới D không. Xin trả lời rằng điều này khó xảy ra hơn vì nếu so với đôc DE thì dốc CE cao hơn nhiều.\n",
    "\n",
    "Dựa trên hiện tượng này, một thuật toán được ra đời nhằm khắc phục việc nghiệm của GD rơi vào một điểm cực tiểu không mong muốn. Thuật toán đó có tên là Momentum (tức _theo đà_ trong tiếng Việt).\n",
    "\n",
    "### Gradient Descent với Momentum\n",
    "Để biểu diễn _momentum_ bằng toán học thì chúng ta phải làm thế nào?\n",
    "\n",
    "Trong GD, chúng ta cần tính lượng thay đổi ở thời điểm \\\\(t\\\\) để cập nhật vị trí mới cho nghiệm (tức _hòn bi_). Nếu chúng ta coi đại lượng này như vận tốc \\\\(v_t\\\\) trong vật lý, vị trí mới của _hòn bi_ sẽ là \\\\(\\theta\\_{t+1} = \\theta\\_{t} - v\\_t\\\\). Dấu trừ thể hiện việc phải di chuyển ngược với đạo hàm. Công việc của chúng ta bây giờ là tính đại lượng \\\\(v\\_t\\\\) sao cho nó vừa mang thông tin của _độ dốc_ (tức đạo hàm), vừa mang thông tin của _đà_, tức vận tốc trước đó \\\\(v\\_{t-1}\\\\) (chúng ta coi như vận tốc ban đầu \\\\(v\\_0=0\\\\)). Một cách đơn giản nhất, chúng ta cộng (có trọng số) hai đại lượng này lại:\n",
    "\\\\[\n",
    "v\\_{t}= \\gamma v\\_{t-1} + \\eta \\nabla\\_{\\theta}J(\\theta)\n",
    "\\\\]\n",
    "\n",
    "Trong đó \\\\(\\gamma\\\\) thường được chọn là một giá trị khoảng 0.9. Sau đó vị trí mới của _hòn bi_ được xác định như sau:\n",
    "\\\\[\n",
    "\\theta = \\theta - v_t\n",
    "\\\\]\n",
    "\n",
    "Thuật toán đơn giản này tỏ ra rất hiệu quả trong các bài toán thực tế (trong không gian nhiều chiều, cách tính toán cũng hoàn tòan tương tự). Dưới đây là một ví dụ trong không gian một chiều.\n",
    "\n",
    "#### Một ví dụ nhỏ\n",
    "Chúng ta xem xét một hàm đơn giản có hai điểm local minimum, trong đó 1 điểm là global minimum:\n",
    "\\\\[\n",
    "f(x) = x^2 + 10\\sin(x)\n",
    "\\\\]\n",
    "Có đạo hàm là: \\\\(f'(x) = 2x + 10\\cos(x)\\\\). Hình 2 dưới đây thể hiện sự khác nhau giữa thuật toán GD và thuật toán GD với Momentum:\n",
    "\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\"> \n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/nomomentum1d.gif\">\n",
    "         </td>\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\">\n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/momentum1d.gif\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình bên trái là đường đi của nghiệm khi không sử dụng Momentum, thuật toán hội tụ sau chỉ 5 vòng lặp nhưng nghiệm tìm được là nghiệm local minimun.\n",
    "\n",
    "Hình bên phải là đường đi của nghiệm khi có sử dụng Momentum, _hòn bi_ đã có thể vượt dốc tới khu vực gần điểm global minimun, sau đó dao động xung quanh điểm này, giảm tốc rồi cuối cùng tới đích. Mặc dù mất nhiều vòng lặp hơn, GD với Momentum cho chúng ta nghiệm chính xác hơn. Quan sát đường đi của _hòn bi_ trong trường hợp này, chúng ta thấy rằng điều này giống với vật lý hơn!\n",
    "\n",
    "Nếu biết trước điểm _đặt bi_ ban đầu `theta`, đạo hàm của hàm mất mát tại một điểm bất kỳ `grad(theta)`, `gamma` và learning rate `eta`, chúng ta có thể viết hàm số `GD_momentum` trong Python như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_converged(theta_new, grad):\n",
    "    return np.linalg.norm(grad(theta_new))/\n",
    "                            len(theta_new) < 1e-3\n",
    "\n",
    "def GD_momentum(theta_init, grad, eta, gamma):\n",
    "    # Suppose we want to store history of theta\n",
    "    theta = [theta_init]\n",
    "    v_old = np.zeros_like(theta_init)\n",
    "    for it in range(100):\n",
    "        v_new = gamma*v_old + eta*grad(theta[-1])\n",
    "        theta_new = theta[-1] - v_new\n",
    "        if has_converge(theta_new, grad):\n",
    "            break \n",
    "        w.append(theta_new)\n",
    "        v_old = v_new\n",
    "    return theta \n",
    "    # this variable includes every points in the path\n",
    "    # if you just want the final answer, \n",
    "    # use `return theta[-1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method\n",
    "Nhắc lại rằng, cho tới thời điểm này, chúng ta luôn giải phương trình đạo hàm của hàm mất mát bằng 0 để tìm các điểm local minimun. (Và trong nhiều trường hợp, coi nghiệm tìm được là nghiệm của bài toán tìm giá trị nhỏ nhất của hàm mất mát). Có một thuật toán nối tiếng giúp giải bài toán \\\\(f(x) = 0\\\\), thuật toán đó có tên là Newton's method.\n",
    "\n",
    "\n",
    "#### Newton's method cho giải phương trình \\\\(f(x) = 0\\\\)\n",
    "\n",
    "Thuật toán Newton's method được mô tả trong hình động minh họa dưới đây:\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif\" align = \"center\" width = \"500\">\n",
    " <div class = \"thecap\"> Minh họa thuật toán Newton's method trong giải phương trình. (Nguồn: <a = href = \"https://en.wikipedia.org/wiki/Newton's_method\">Wikipedia</a>).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ý tưởng giải bài toán \\\\(f(x) = 0\\\\) bằng phương pháp Newton's method như sau. Chọn một điểm \\\\(x\\_0\\\\) mà ta dự đoán là gần với nghiệm. Sau đó vẽ đường tiếp tuyến (mặt tiếp tuyến trong không gian nhiều chiều) với đồ thị hàm số \\\\(y = f(x)\\\\) tại điểm trên đồ thị có hoành độ \\\\(x_0\\\\). Giao điểm \\\\(x_1\\\\) của đường tiếp tuyến này với trục hoành được xem như gần với nghiệm hơn. Thuật toán lặp lại với điểm mới \\\\(x_1\\\\) và cứ như vậy đến khi ta được \\\\(f(x_t) \\approx 0\\\\).\n",
    "\n",
    "Đó là ý nghĩa hình học của Newton's method, chúng ta cần một công thức để có thể dựa vào đó để lập trình. Việc này không quá phức tạp với các bạn thi đại học môn toán ở VN. Thật vậy, phương trình tiếp tuyến với đồ thị của hàm \\\\(f(x)\\\\) tại điểm có hoành độ \\\\(x_t\\\\) là:\n",
    "\\\\[\n",
    "y = f'(x)(x - x_t) + f(x_t)\n",
    "\\\\]\n",
    "Giao điểm của đường thẳng này với trục \\\\(x\\\\) tìm được bằng cách giải phương trình vế phải của biểu thức trên bằng 0, tức là:\n",
    "\\\\[\n",
    "x = x\\_t - \\frac{f(x\\_t)}{f'(x\\_t)} \\triangleq x\\_{t+1}\n",
    "\\\\]\n",
    "\n",
    "### Newton's method trong bài toán tìm local minimun\n",
    "Áp dụng phương pháp này cho việc giải phương trình \\\\(f'(x) = 0\\\\) ta có:\n",
    "\\\\[\n",
    "x\\_{t+1} = x\\_t - \\frac{1}{f''(x_t)}{f'(x_t)}\n",
    "\\\\]\n",
    "\n",
    "Và trong không gian nhiều chiều với \\\\(\\theta\\\\) là biến:\n",
    "\\\\[\n",
    "\\theta = \\theta - \\frac{1}{\\|\\nabla^2\\_{\\theta} J(\\theta)\\|} \\nabla\\_{\\theta} J(\\theta)\n",
    "\\\\]\n",
    "trong đó \\\\(\\nabla^2\\_{\\theta} J(\\theta)\\\\) là đạo hàm bậc hai của hàm mất mất. Biểu thức này là một ma trận nếu \\\\(\\theta\\\\) là một vector. Và \\\\(\\|\\nabla^2\\_{\\theta} J(\\theta)\\|\\\\) chính là định thức (determinant) của ma trận đó. \n",
    "\n",
    "Bạn đọc có thể nhận thấy rằng đây chính là trường hợp đặc biệt của Gradient Descent với learning rate được tính chính xác:\n",
    "\\\\[\n",
    "eta = \\frac{1}{\\|\\nabla^2\\_{\\theta} J(\\theta)\\|}\n",
    "\\\\]\n",
    "\n",
    "Nếu có một phương pháp hiệu quả để tính \\\\(\\|\\nabla^2\\_{\\theta} J(\\theta)\\|\\\\), Newton's method thường cho nghiệm sau ít vòng lặp hơn so với GD vì tại mỗi vòng lặp, nó cho biết chính xác _quãng đường cần di chuyển_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hạn chế của Newton's method\n",
    "1. Điểm khởi tạo phải *rất* gần với nghiệm \\\\(x^\\*\\\\).\n",
    "Ý tưởng sâu xa hơn của Newton's method là dựa trên khai triển Taylor của hàm số \\\\(f(x)\\\\) tới đạo hàm thứ nhất:\n",
    "\\\\[\n",
    "0 = f(x^\\*) \\approx f(x\\_t) + f'(x\\_t)(x\\_t - x^\\*)\n",
    "\\\\]\n",
    "Từ đó suy ra: \\\\(x^\\* \\approx x_t - \\frac{f(x_t)}{f'(x_t)}\\\\). \n",
    "Một điểm rất quan trọng, khai triển Taylor chỉ đúng nếu \\\\(x_t\\\\) rất gần với \\\\(x^\\*\\\\)!\n",
    "Dưới đây là một ví dụ kinh điển trên Wikipedia về việc Newton's method cho một dãy số phân kỳ (divergence).\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/NewtonsMethodConvergenceFailure.svg/300px-NewtonsMethodConvergenceFailure.svg.png\" align = \"center\" width = \"400\">\n",
    " <div class = \"thecap\"> Nghiệm là một điểm gần -2. Tiếp tuyến của đồ thị hàm số tại điểm có hoành độ bằng 0 cắt trục hoành tại 1, và ngược lại. Trong trường hợp này, Newton's method không bao giờ hội tụ. (Nguồn: <a = href = \"https://en.wikipedia.org/wiki/Newton's_method\">Wikipedia</a>).\n",
    "</div>\n",
    "\n",
    "2. Nhận thấy rằng trong việc giải phương trình \\\\(f(x) = 0\\\\), chúng ta có đạo hàm ở mẫu số. Khi đạo hàm này gần với 0, ta sẽ được một đường thằng song song hoặc gần song song với trục hoành. Ta sẽ hoặc không tìm được giao điểm, hoặc được một giao điểm ở vô cùng. Đặc biệt, khi nghiệm chính là điểm có đạo hàm bằng 0, thuật toán gần như sẽ không tìm được nghiệm!\n",
    "\n",
    "3. Khi áp dụng Newton's method cho thuật toán GD, chúng ta cần tính định thức của đạo hàm bậc hai. Khi số chiều và số điểm dữ liệu lớn, đạo hàm bậc hai của hàm mất mát sẽ là một ma trận rất lớn, ảnh hưởng tới cả memory và tốc độ tính toán của hệ thống. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các thuật toán khác\n",
    "Ngoài hai thuật toán trên, có rất nhiều thuật toán nâng cao khác được sử dụng trong các bài toán thực tế, đặc biệt là các bài toán Deep Learning. Có thể nêu một vài từ khóa như Nesterov accelerated gradient, Adagrad, Adam, RMSprop,... Để tránh việc độc giả bị ngợp, tôi sẽ không đề cập đến các thuật toán đó trong bài này mà sẽ dành thời gian nói tới nếu có dịp trong tương lai, khi blog đã đủ lớn và đã trang bị cho các bạn một lượng kiến thức nhất định. \n",
    "\n",
    "Tuy nhiên, bạn đọc nào muốn đọc thêm có thể tìm được rất nhiều thông tin hữu ích trong bài này:\n",
    "[An overview of gradient descent optimization algorithms ](http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biến thể của Gradient Descent\n",
    "Tôi xin một lần nữa dùng bài toán [Linear Regression](/2016/12/28/linearregression/) làm ví dụ. Hàm mất mát và đạo hàm của nó cho bài toán này lần lượt là (để cho thuận tiện, trong bài này tôi sẽ dùng ký hiệu \\\\(\\mathbf{X}\\\\) thay cho dữ liệu mở rộng \\\\(\\bar{\\mathbf{X}}\\\\)):\n",
    "\n",
    "\\\\[\n",
    "J(\\mathbf{w}) = \\frac{1}{2N}\\|\\|\\mathbf{X}\\mathbf{w} - y\\|\\|\\_2^2\n",
    "\\\\]\n",
    "\\\\[\n",
    "~~~~ = \\frac{1}{2N} \\sum\\_{i=1}^N(\\mathbf{\\bar{x}}\\_i \\mathbf{w} - y\\_i)^2\n",
    "\\\\]\n",
    "và:\n",
    "\\\\[\n",
    "\\nabla\\_{\\mathbf{w}} J(\\mathbf{w}) = \\frac{1}{N}\\sum\\_{i=1}^N \\mathbf{x}\\_i^T(\\mathbf{x}_i\\mathbf{w} - y\\_i)\n",
    "\\\\]\n",
    "\n",
    "### Batch Gradient Descent \n",
    "Thuật toán Gradient Descent chúng ta nói từ đầu phần 1 đến giờ còn được gọi là Batch Gradient Descent. Batch ở đây được hiểu là _tất cả_, tức khi cập nhật \\\\(\\theta = \\mathbf{w}\\\\), chúng ta sử dụng **tất cả** các điểm dữ liệu \\\\(\\mathbf{x}_i\\\\). \n",
    "\n",
    "Cách làm này có một vài hạn chế đối với cơ sở dữ liệu có vô cùng nhiều điểm (hơn 1 tỉ người dùng của facebook chẳng hạn). Việc phải tính toán lại đạo hàm với tất cả các điểm này sau mỗi vòng lặp trở nên cồng kềnh và không hiệu quả. Thêm nữa, thuật toán này được coi là không hiệu quả với _online learning_.\n",
    "\n",
    "**Online learning** là khi cơ sở dữ liệu được cập nhật liên tục, mỗi lần thêm vài điểm dữ liệu mới. Kéo theo đó là mô hình của chúng ta cũng phải thay đổi một chút để phù hợp với các dữ liệu mới này. Nếu làm theo Batch Gradient Descent, tức tính lại đạo hàm của hàm mất mát tại tất cả các điểm dữ liệu, thì thời gian tính toán sẽ rất lâu, và thuật toán của chúng ta coi như không _online_ nữa do mất quá nhiều thời gian tính toán.\n",
    "\n",
    "Trên thực tế, có một thuật toán đơn giản hơn và tỏ ra rất hiệu quả, có tên gọi là Stochastic Gradient Descent (SGD).\n",
    "### Stochastic Gradient Descent.\n",
    "Trong thuật toán này, tại 1 thời điểm, ta chỉ tính đạo hàm của hàm mất mát dựa trên _chỉ một_ điểm dữ liệu \\\\(\\mathbf{x_i}\\\\) rồi cập nhật \\\\(\\theta\\\\) dựa trên đạo hàm này. Việc này được thực hiện trên toàn bộ dữ liệu, sau đó lặp lại quá trình trên. Thuật toán rất đơn giản này trê thực tế lại làm việc rất hiệu quả. \n",
    "\n",
    "Mỗi lần duyệt qua tất cả các điểm trên toàn bộ dữ liệu được gọi là một epoch. Với GD thông thường thì mỗi epoch ứng với 1 lần cập nhận \\\\(\\theta\\\\), với SGD thì mỗi epoch ứng với \\\\(N\\\\) lần cập nhật \\\\(\\theta\\\\) với \\\\(N\\\\) là số điểm dữ liệu. Nhìn vào một mặt, việc cập nhật từng điểm một như thế này có thể làm giảm đi tốc độ thực hiện 1 lần trên toàn bộ dữ liệu. Nhưng nhìn vào một mặt khác, SDG chỉ yêu cầu một lượng epoch rất nhỏ (thường là 10 cho lần đầu tiên, sau đó khi có dữ liệu mới thì chỉ cần chạy dưới một epoch là đã có nghiệm tốt). Vì vậy SGD phù hợp với các bài toán có lượng cơ sở dữ liệu lớn (chủ yếu là Deep Learning mà chúng ta sẽ thấy trong phần sau của blog) và các baì toán yêu cầu mô hình thay đổi liên tục, tức online learning. \n",
    "\n",
    "**Thứ tự lựa chọn điểm dữ liệu**\n",
    "Một điểm cần lưu ý đó là: sau mỗi epoch, chúng ta cần shuffle (xáo trộn) thứ tự của các dữ liệu để đảm bảo tính ngẫu nhiên. Việc này cũng ảnh hưởng tới hiệu năng của SGD. \n",
    "\n",
    "\n",
    "Một cách toán học, quy tắc cập nhật của SGD là:\n",
    "\\\\[\n",
    "\\theta = \\theta - \\eta \\nabla\\_{\\theta} J(\\theta; \\mathbf{x}_i; \\mathbf{y}_i)\n",
    "\\\\]\n",
    "\n",
    "trong đó \\\\(J(\\theta; \\mathbf{x}_i; \\mathbf{y}_i)\\\\) là hàm mất mát với chỉ 1 cặp điểm dữ liệu (input, label) là (\\\\(\\mathbf{x}_i, \\mathbf{y}_i\\\\)). **Chú ý:** chúng ta hoàn toàn có thể áp dụng các thuật toán tăng tốc GD như Momentum, AdaGrad,... vào SGD.\n",
    "\n",
    "#### Ví dụ với bài toán Linear Regression\n",
    "Với bài toán Linear Regression, \\\\(\\theta = \\mathbf{w}\\\\), hàm mất mát tại một điểm dữ liệu là:\n",
    "\\\\[\n",
    "J(\\mathbf{w}; \\mathbf{x}\\_i; y\\_i) = \\frac{1}{2}(\\mathbf{x}^i \\mathbf{w} - y\\_i)^2\n",
    "\\\\]\n",
    "Đạo hàm tương ứng là:\n",
    "\\\\[\n",
    "\\nabla\\_{\\mathbf{w}}J(\\mathbf{w}; \\mathbf{x}\\_i; y\\_i) = \\mathbf{x}\\_i^T(\\mathbf{x}\\_i \\mathbf{w} - y_i)^2\n",
    "\\\\]\n",
    "Và dưới đây là hàm số trong python để giải Linear Regression theo SGD:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# single point gradient\n",
    "def sgrad(w, i, rd_id):\n",
    "    true_i = rd_id[i]\n",
    "    xi = Xbar[true_i, :]\n",
    "    yi = y[true_i]\n",
    "    a = np.dot(xi, w) - yi\n",
    "    return (xi*a).reshape(2, 1)\n",
    "\n",
    "def SGD(w_init, grad, eta):\n",
    "    w = [w_init]\n",
    "    w_last_check = w_init\n",
    "    iter_check_w = 10\n",
    "    N = X.shape[0]\n",
    "    count = 0\n",
    "    for it in range(10):\n",
    "        # shuffle data \n",
    "        rd_id = np.random.permutation(N)\n",
    "        for i in range(N):\n",
    "            count += 1 \n",
    "            g = sgrad(w[-1], i, rd_id)\n",
    "            w_new = w[-1] - eta*g\n",
    "            w.append(w_new)\n",
    "            if count%iter_check_w == 0:\n",
    "                w_this_check = w_new                 \n",
    "                if np.linalg.norm(w_this_check - w_last_check)/len(w_init) < 1e-3:                                    \n",
    "                    return w\n",
    "                w_last_check = w_this_check\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả được cho như hình dưới đây ([với dữ liệu được tạo giống như ở phần 1](/2017/01/12/gradientdescent/#quay-lai-voi-bai-toan-linear-regression)).\n",
    "\n",
    "<table width = \"100%\" style = \"border: 0px solid white\">\n",
    "   <tr >\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\"> \n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/LR_SGD_contours.gif\">\n",
    "         </td>\n",
    "        <td width=\"40%\" style = \"border: 0px solid white\">\n",
    "        <img style=\"display:block;\" width = \"100%\" src = \"/assets/GD/LR_SGD_loss.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "Hình bên trái mô tả đường đi của nghiệm. Chúng ta thấy rằng đường đi khá là _zigzag_ chứ không _mượt_ như khi sử dụng GD. Điều này là dễ hiểu vì một điểm dữ liệu không thể đại diện cho toàn bộ dữ liệu được. Tuy nhiên, chúng ta cũng thấy rằng thuật toán hội tụ khá nhanh đến vùng lân cận của nghiệm. Với 1000 điểm dữ liệu, SGD chỉ cần gần 3 epoches (2911 tương ứng với 2911 lần cập nhật, mỗi lần lấy 1 điểm). Nếu so với con số 49 vòng lặp (epoches) như kết quả tốt nhất có được băngf GD, thì kết quả này lợi hơn rất nhiều. \n",
    "\n",
    "Hình bên phải mô tả hàm mất mát cho toàn bộ dữ liệu sau khi _ch_ sử dụng 50 điểm dữ liệu đầu tiên. Mặc dù không _mượt_, tốc độ hội tụ vẫn rất nhanh. \n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "Khác với SGD, mini-batch sử dụng một số lượng \\\\(n\\\\) lớn hơn 1 (nhưng vẫn nhỏ hơn tổng số dữ liệu \\\\(N\\\\)rất nhiều). Giống với SGD, Mini-batch Gradient Descent bắt đầu mỗi epoch bằng việc xáo trộn ngẫu nhiên dữ liệu rồi chia toàn bộ dữ liệu thành các _mini-batch_, mỗi _mini-batch_ có \\\\(n\\\\) điểm dữ liệu (trừ mini-batch cuối có thể có ít hơn nếu \\\\(N\\\\) không chia hết cho \\\\(n\\\\)). Mỗi lần cập nhật, thuật toán này lấy ra một mini-batch để tính toán đạo hàm rồi cập nhật. Công thức có thể viết dưới dạng:\n",
    "\\\\[\n",
    "\\theta = \\theta - \\eta\\nabla\\_{\\theta} J(\\theta; \\mathbf{x}\\_{i:i+n}; \\mathbf{y}\\_{i:i+n})\n",
    "\\\\]\n",
    "Với \\\\(\\mathbf{x}\\_{i:i+n}\\\\) được hiểu là dữ liệu từ thứ \\\\(i\\\\) tới thứ \\\\(i+n-1\\\\) (theo ký hiệu của Python). Dữ liệu này sau mỗi epoch là khác nhau vì chúng đã bị xáo trộn. Một lần nữa, các thuật toán khác cho GD như Momentum, Adagrad, Adadelta,... cũng có thể được áp dụng vào đây.\n",
    "\n",
    "Mini-batch GD được sử dụng trong hầu hết các thuật toán Machine Learning, đặc biệt là trong Deep Learning. Giá trị \\\\(n\\\\) thường được chọn là khoảng từ 50 đến 100.\n",
    "\n",
    "Dưới đây là ví dụ về giá trị của hàm mất mát mỗi khi cập nhật tham số \\\\(\\theta\\\\) của một bài toán khác phức tạp hơn.\n",
    "\n",
    "<div class=\"imgcap\">\n",
    " <img src =\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png\" align = \"center\" width = \"400\">\n",
    " <div class = \"thecap\"> Hàm mất mát _nhảy lên nhảy xuống_ (fluctuate) sau mỗi lần cập nhật nhưng nhìn chung giảm dần và có xu hướng hội tụ về cuối. (Nguồn: <a = href = \"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">Wikipedia</a>).\n",
    "</div>\n",
    "\n",
    "Để có thêm thông tin chi tiết hơn, bạn đọc có thể tìm trong [bài viết rất tốt này](http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Criteria (điều kiện dừng)\n",
    "\n",
    "Có một điểm cũng quan trọng mà từ đầu tôi chưa nhắc đến: khi nào thì chúng ta biết thuật toán đã hội tụ và dừng lại?\n",
    "\n",
    "Trong thực nghiệm, có một vài phương pháp như dưới đây:\n",
    "\n",
    "1. Giới hạn số vòng lặp: đây là phương pháp phổ biến nhất và cũng để đảm bảo rằng chương trình chạy không quá lâu. Tuy nhiên, một nhược điểm của cách làm này là có thể thuật toán dừng lại trước khi đủ gần với nghiệm. \n",
    "2. So sánh gradient của nghiệm tại hai lần cập nhật liên tiếp, khi nào giá trị này đủ nhỏ thì dừng lại. Phương pháp này cũng có một nhược điểm lớn là việc tính đạo hàm đôi khi trở nên quá phức tạp (ví dụ như khi có quá nhiều dữ liệu), nếu áp dụng phương pháp này thì coi như ta không được lợi khi sử dụng SGD và mini-batch GD. \n",
    "3. So sánh giá trị của hàm mất mát của nghiệm tại hai lần cập nhật liên tiếp, khi nào giá trị này đủ nhỏ thì dừng lại. Nhược điểm của phương pháp này là nếu tại một thời điểm, đồ thị hàm số có dạng _bẳng phẳng_ tại một khu vực nhưng khu vực đó không chứa điểm local minimum (khu vực này thường được gọi là saddle points), thuật toán cũng dừng lại trước khi đạt giá trị mong muốn. \n",
    "4. Trong SGD và mini-batch GD, cách thường dùng là so sánh nghiệm sau một vài lần cập nhật. Trong đoạn code Python phía trên về SGD, tôi áp dụng việc so sánh này mỗi khi nghiệm được cập nhật 10 lần. Việc làm này cũng tỏ ra khá hiệu quả. \n",
    "\n",
    "\n",
    "## Kết luận\n",
    "Qua hai bài viết về Gradient Descent này, tôi hy vọng các bạn đã hiểu và làm quen với một thuật toán tối ưu được sử dụng nhiều nhất trong Machine Learning và đặc biệt là Deep Learning. Còn nhiều biến thể khác khá thú vị về GD (mà rất có thể tôi chưa biết tới), nhưng tôi xin phép được dừng chuỗi bài về GD tại đây và tiếp tục chuyển sang các thuật toán thú vị khác. \n",
    "\n",
    "Hy vọng bài viết có ích với các bạn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Newton's method - Wikipedia](https://en.wikipedia.org/wiki/Newton's_method)\n",
    "2. [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent)\n",
    "3. [Stochastic Gradient Descent - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "4. [Stochastic Gradient Descen - Andrew Ng](https://www.youtube.com/watch?v=UfNU3Vhv5CA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
